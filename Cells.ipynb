{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cells.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q87duyVBy4AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvf dataset.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgrxLS5sxLWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "outputId": "e5a0c85a-9ba1-4300-b0db-75122d24ead6"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#from tensorflow.keras.applications import ResNet50,ResNet50V2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_shape=(128,128,3)\n",
        "batch_size = 20\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', \n",
        "                  input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
        "        BatchNormalization(),        \n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        \n",
        "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
        "        BatchNormalization(),    \n",
        "        MaxPooling2D(pool_size=(2, 2)),   \n",
        "        \n",
        "        Flatten(),\n",
        "        \n",
        "        Dense(1024, activation='relu'),\n",
        "        \n",
        "        Dense(512, activation='relu'),\n",
        "        \n",
        "        Dense(1, activation='sigmoid')\n",
        "        \n",
        "    ])\n",
        "    opt = Adam(lr=1e-6)\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def training():\n",
        "    # define model\n",
        "    model = build_model()\n",
        "    # create data generator\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        rescale=1.0/255.,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1)\n",
        "\n",
        "    validation_datagen = ImageDataGenerator( rescale = 1.0/255.)\n",
        "    test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "    # prepare iterators\n",
        "    train_generator = train_datagen.flow_from_directory('output/train/',\n",
        "                                                 class_mode='binary',\n",
        "                                                 batch_size=batch_size, \n",
        "                                                 target_size=input_shape[:2])\n",
        "    test_generator = test_datagen.flow_from_directory('output/test/',\n",
        "                                               class_mode='binary', \n",
        "                                               batch_size=batch_size, \n",
        "                                               target_size=input_shape[:2])\n",
        "    validation_generator = validation_datagen.flow_from_directory('output/val/',\n",
        "                                                        class_mode='binary',\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        target_size=input_shape[:2])\n",
        "    steps_per_epoch = train_generator.n // batch_size\n",
        "    validation_steps = validation_generator.n // batch_size\n",
        "    earlystop = EarlyStopping(monitor='val_accuracy', \n",
        "                              min_delta=0.05, \n",
        "                              patience=10, \n",
        "                              verbose=1, \n",
        "                              mode='auto')\n",
        "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                patience=10,\n",
        "                                                verbose=1,\n",
        "                                                factor=0.5,\n",
        "                                                min_lr=0.00001,\n",
        "                                                mode='auto'\n",
        "                                               )\n",
        "    checkpoint_filepath = 'weights/'\n",
        "    model_checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    callbacks_list = [earlystop, learning_rate_reduction,model_checkpoint_callback]\n",
        "    # fit model\n",
        "    history = model.fit(train_generator, \n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_data=validation_generator, \n",
        "                    validation_steps=validation_steps, \n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks_list\n",
        "                   )\n",
        "    # evaluate model\n",
        "    _, acc = model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
        "    print('> %.3f' % (acc * 100.0))\n",
        "    # learning curves\n",
        "    summarize_diagnostics(history)\n",
        "\n",
        "def summarize_diagnostics(history):\n",
        "\t\n",
        "    acc      = history.history['accuracy']\n",
        "    val_acc  = history.history['val_accuracy']\n",
        "    loss     = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs   = range(1,len(acc)+1,1)\n",
        "    plt.subplot(211)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot ( epochs,     acc, 'r--', label='Training acc'  )\n",
        "    plt.plot ( epochs, val_acc,  'b', label='Validation acc')\n",
        "    plt.title ('Training and validation accuracy')\n",
        "    plt.ylabel('acc')\n",
        "    plt.xlabel('epochs')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    \n",
        "    plt.subplot(212)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot ( epochs,     loss, 'r--' )\n",
        "    plt.plot ( epochs, val_loss ,  'b' )\n",
        "    plt.title ('Training and validation loss'   )\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epochs')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.show()\n",
        "if __name__=='__main__':\n",
        "    training()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 128, 128, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 126, 126, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 126, 126, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 63, 63, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 63, 63, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 63, 63, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 61, 61, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 61, 61, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 30, 30, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 30, 30, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 30, 30, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 28, 28, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1024)              51381248  \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 53,055,553\n",
            "Trainable params: 53,053,761\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "Found 6000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "261/300 [=========================>....] - ETA: 4:56 - loss: 0.5291 - acc: 0.7454"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}